behaviors:
  Devil:
    trainer_type: ppo
    hyperparameters:
      batch_size: 64                 # Number of training examples in a batch
      buffer_size: 2048              # Experience buffer size
      learning_rate: 3.0e-4          # Initial learning rate
      learning_rate_schedule: linear # Gradual reduction of learning rate
      beta: 1.0e-5                   # Lower entropy regularization to focus on exploitation
      beta_schedule: constant        # Keep low exploration constant
      epsilon: 0.05                  # Reduced PPO clipping range for tighter policy updates
      epsilon_schedule: constant     # Fixed tight updates for exploitation
      lambd: 0.95                    # Slightly lower GAE to weigh immediate rewards more
      num_epoch: 10                  # More passes through the buffer for reinforcement
    network_settings:
      normalize: true                # Normalize observations
      hidden_units: 128              # Number of units in each hidden layer
      num_layers: 2                  # Number of hidden layers in the network
    reward_signals:
      extrinsic:
        gamma: 0.99                  # Discount factor for rewards
        strength: 1.0                # Strength of the reward signal
    max_steps: 1000000               # Total number of training steps
    time_horizon: 128                # Time horizon for trajectory-based training
    summary_freq: 10000              # Frequency (steps) to save Tensorboard summaries
